---
sidebar_position: 2
title: LLM Cognitive Planning
---

# LLM Cognitive Planning

Once a user's command has been transcribed into text by Whisper, the robot needs to understand the command and create a plan to execute it. This is where a **Large Language Model (LLM)** comes in. An LLM can act as the "brain" of the robot, performing cognitive planning by breaking down a high-level natural language command into a sequence of concrete, executable actions.

## The Role of LLMs in Robotics

LLMs like GPT-4 are incredibly powerful at reasoning, planning, and understanding context. In a robotics context, they can be used to:
-   **Parse Intent**: Understand the user's goal from a natural language command (e.g., "bring me the red cup from the table").
-   **Decompose Tasks**: Break down a complex task into a series of smaller, more manageable sub-tasks.
-   **Generate Actionable Plans**: Convert the sub-tasks into a sequence of robot actions that can be executed by the robot's control system.
-   **Incorporate World Knowledge**: Use their vast training data to reason about objects and concepts in the world (e.g., knowing that a cup is something that can be grasped).

## Few-Shot Prompting for Robotic Planning

To get an LLM to generate a plan for a robot, we can use a technique called **few-shot prompting**. This involves giving the LLM a prompt that includes:
1.  A high-level description of the robot's capabilities (the available actions).
2.  A few examples of high-level commands and the corresponding low-level action plans.
3.  The new command that needs to be planned.

By providing examples, we constrain the LLM's output to the specific format and actions that our robot understands.

### Example Prompt for an LLM Planner

Here is a simplified example of a prompt we could send to an LLM like GPT-4.

```text
You are a helpful AI assistant for a humanoid robot. You need to translate a high-level user command into a sequence of simple, low-level actions that the robot can execute.

The available robot actions are:
- `moveTo(location)`: Moves the robot to a specific location (e.g., 'table', 'kitchen_counter').
- `findObject(object_name, location)`: Looks for an object in a specific location.
- `graspObject(object_name)`: Grasps the specified object.
- `placeObject(location)`: Places the currently held object at the specified location.
- `say(text)`: Says the given text.

Here are a few examples:

Command: "Get me the apple from the counter."
Plan:
1. moveTo(kitchen_counter)
2. findObject(apple, kitchen_counter)
3. graspObject(apple)
4. say("I have the apple.")

Command: "Can you put the blue block on the table?"
Plan:
1. findObject(blue_block, user_hand)
2. graspObject(blue_block)
3. moveTo(table)
4. placeObject(table)
5. say("I have placed the blue block on the table.")

Now, generate a plan for the following command.

Command: "Bring me the water bottle from the desk."
Plan:
```

The LLM would then complete the prompt with a plan like:
```
1. moveTo(desk)
2. findObject(water_bottle, desk)
3. graspObject(water_bottle)
4. say("I have the water bottle.")
```

## Integrating the LLM Planner into ROS 2

We can create a ROS 2 node that:
1.  Subscribes to the `transcribed_text` topic from the Whisper node.
2.  When it receives a new command, it constructs a prompt like the one above.
3.  Sends the prompt to an LLM API (e.g., the OpenAI API).
4.  Receives the generated plan from the LLM.
5.  Parses the plan and publishes the individual actions to another topic (e.g., an action server that executes the robot's actions).

## Next Steps

With a plan generated by the LLM, the final step in the VLA pipeline is to execute that plan. This involves a multi-modal action pipeline that can translate the symbolic actions from the plan (`moveTo`, `graspObject`, etc.) into actual robot movements and behaviors. This will be the focus of the next chapter.
