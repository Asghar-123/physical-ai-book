---
sidebar_position: 1
title: Whisper Speech-to-Command
---

# Whisper Integration for Speech-to-Command

Integrating speech recognition into a humanoid robot opens up a new dimension of intuitive human-robot interaction. **OpenAI's Whisper** is a state-of-the-art automatic speech recognition (ASR) model that can transcribe spoken language into text with high accuracy. By integrating Whisper, we can create a speech-to-command pipeline, allowing users to control a robot using natural language.

## The Role of Whisper in a VLA Pipeline

In a Vision-Language-Action (VLA) pipeline, Whisper serves as the "ears" of the robot. It captures spoken commands from a user, transcribes them into text, and passes the text to the next stage of the pipeline, which is typically a Large Language Model (LLM) for cognitive planning.

## Setting up Whisper

Whisper can be used in two main ways:
1.  **Via the OpenAI API**: This is the easiest way to get started. You send an audio file to the API and get back the transcribed text. This requires an internet connection and an OpenAI API key.
2.  **Locally**: You can run Whisper models locally on your own machine. This is more private and doesn't require an internet connection, but it does require a machine with sufficient computational resources (preferably a GPU).

For a robotics application, running Whisper locally is often preferred to minimize latency and ensure operation in environments without internet access.

## Creating a ROS 2 Whisper Node

To integrate Whisper into our ROS 2 ecosystem, we can create a node that:
1.  Subscribes to an audio topic (e.g., `sensor_msgs/AudioData`).
2.  Receives audio data from a microphone attached to the robot or in the environment.
3.  Uses the Whisper model to transcribe the audio data into text.
4.  Publishes the transcribed text to a string topic (e.g., `std_msgs/String`).

### Example Python ROS 2 Node for Whisper

Here is a conceptual Python script for a ROS 2 node that uses a local Whisper model. This example assumes you have the `whisper` Python package installed (`pip install openai-whisper`).

```python
# Conceptual Whisper ROS 2 Node
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import AudioData
import whisper
import numpy as np

class WhisperNode(Node):
    def __init__(self):
        super().__init__('whisper_node')
        self.subscription = self.create_subscription(
            AudioData,
            'audio',
            self.audio_callback,
            10)
        self.publisher = self.create_publisher(String, 'transcribed_text', 10)
        self.model = whisper.load_model("base.en") # Load the base English model

    def audio_callback(self, msg):
        self.get_logger().info('Received audio data...')
        # Convert the audio data to a numpy array that Whisper can process
        audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0

        # Transcribe the audio
        result = self.model.transcribe(audio_data)
        transcribed_text = result["text"]
        self.get_logger().info(f'Whisper transcribed: "{transcribed_text}"')

        # Publish the transcribed text
        text_msg = String()
        text_msg.data = transcribed_text
        self.publisher.publish(text_msg)

def main(args=None):
    rclpy.init(args=args)
    whisper_node = WhisperNode()
    rclpy.spin(whisper_node)
    whisper_node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

This node listens for audio data, transcribes it, and then publishes the resulting text. This text can then be used by other nodes in the system.

## Next Steps

Once a spoken command has been transcribed into text, the next step is to understand the user's intent and generate a plan to accomplish the requested task. This is the role of the LLM cognitive planning module, which will be discussed in the next chapter.
